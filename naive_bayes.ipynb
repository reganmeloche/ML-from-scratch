{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3ox662y2085B",
        "WR-o0bpl3n9R",
        "XyAAUwjYLAgE",
        "cQXVQ8LiOJkj",
        "EwBXFLjn6WLr",
        "3yXoeEzEEHCG",
        "vKTHM-1rGSj7"
      ],
      "authorship_tag": "ABX9TyODIf62ND3rsDVFZbyf6naD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reganmeloche/ML-from-scratch/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n"
      ],
      "metadata": {
        "id": "3ox662y2085B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources\n",
        "- https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer\n",
        "- https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
        "- https://machinelearningmastery.com/bayes-theorem-for-machine-learning/\n",
        "- Udemy course: https://www.udemy.com/course/machinelearning/"
      ],
      "metadata": {
        "id": "ZMpLU2yU0-yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Ucm5DdP0PRFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory\n"
      ],
      "metadata": {
        "id": "WR-o0bpl3n9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formula for calculating conditional probability.\n",
        "\n",
        "Marginal probability: Probability of an event irrespective of outcomes of other random variables\n",
        "- P(A)\n",
        "- aka Prior probability\n",
        "\n",
        "Joint probability: Probability of two or more simultaneous events. \n",
        "- P(A and B) or P(A,B)\n",
        "\n",
        "Conditional probability: Probability of one event given the occurrence of another event,\n",
        "- P(A|B)\n",
        "- aka Posterior probability\n",
        "\n",
        "Rules\n",
        "- P(A,B) = P(A|B) * P(B)\n",
        "- P(A,B) = P(B,A)\n",
        "- P(A|B) = P(A,B) / P(B)\n",
        "- P(A|B) != P(B|A)\n",
        "\n",
        "**Bayes Theorem: P(A|B) = P(B|A) * P(A) / P(B)**\n",
        "\n",
        "Bayes Theorem gives us a way of calculating the conditional probability without having the joint probability.\n",
        "\n",
        "Posterior probablity = Likelihood * Prior / Evidence\n",
        "\n",
        "Example: What is the probability that there is fire, given that there is smoke?\n",
        "- Prior: P(fire)\n",
        "- Likelihood: P(smoke|fire)\n",
        "- evidence: P(smoke)\n",
        "\n",
        "P(fire|smoke) = P(smoke|fire) * P(fire) / P(smoke)"
      ],
      "metadata": {
        "id": "qGI3Ia5F3pok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can frame a classification task as calculating the conditional probablity of a class label given a sample of data:\n",
        "- P(class|data) = P(data|class) * P(class) / P(data)\n",
        "\n",
        "This could be calculated for each class, and the class that is assigned the largest probabiltiy is predicted.\n",
        "\n",
        "But it gets too complicated when we have multiple features, so we need to simplify the calculation. Bayes assumes that each input variable is dependent on all the others, which makes the calculation very complex. If we consider each input variable to be independent of each other, then the calculation is simplified. \n",
        "- P(class|X1, X2, ... , Xn) = [P(X1|class) * P(X2|class) * ... * P(Xn|class)] * P(class) / P(data)\n",
        "\n",
        "This is referred to as **Naive Bayes**."
      ],
      "metadata": {
        "id": "-1ULWbq6I0l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1"
      ],
      "metadata": {
        "id": "XyAAUwjYLAgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario: \n",
        "- Consider a human population that may or may not have cancer\n",
        "- A medical test returns positive or negative for detecting cancer\n",
        "\n",
        "True positive rate is important in medical field - how often is it detected when the patient actually has cancer. Suppose we know it:\n",
        "- P(pos|C) = 0.85\n",
        "\n",
        "Suppose we also know the probability of getting a certain type of cancer:\n",
        "P(C) = 0.02\n",
        "\n",
        "Now we can calculate the probability of a patient having cancer given a positive test using Bayes theorem:\n",
        "- P(A|B) = P(B|A) * P(A) / P(B)\n",
        "- P(C|pos) = P(pos|C) * P(C) / P(pos)\n",
        "- P(C|pos) = 0.85 * 0.0002 / P(pos)\n",
        "\n",
        "To get P(pos), we can estimate\n",
        "- P(pos) = P(pos|C) * P(C) + P(pos|not_C) * P(not_pos)\n",
        "- P(pos) = 0.85 * 0.0002 + P(pos|not_c) * (1 - 0.0002)\n",
        "\n",
        "At this point we need more information. We need to know how good the test is at correctly identifying people who do not have cancer (specificity).\n",
        "- Suppose it is 95%\n",
        "- P(not_pos | not_C)\n",
        "- P(pos | not_C) = 1 - 0.95 = 0.05\n",
        "\n",
        "Plug it all in:\n",
        "- P(pos) = (0.85 * 0.0002) + (0.055 * 0.9998) = 0.05016\n",
        "- probability of returning a positive result is about 5%\n",
        "\n",
        "Now back to Bayes:\n",
        "- P(C|pos) = 0.85 * 0.0002 / 0.05016 = 0.003389\n",
        "- so if a patient tests positive, then there is actually only a 0.33% chance that they have cancer\n",
        "- This is a very bad diagnostic test\n",
        "\n",
        "We often need these pieces of information\n",
        "- Sensitivity: 85% of people with cancer will get a positive result\n",
        "- Base rate: 0.02% of people have cancer\n",
        "- Specificity: 95% of people without cancer will get a negative test result"
      ],
      "metadata": {
        "id": "NfriHFwW58XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2"
      ],
      "metadata": {
        "id": "cQXVQ8LiOJkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 machines that produce a part.\n",
        "- M1: produces 30/hr\n",
        "- M2: produces 20/hr\n",
        "\n",
        "1% of all parts are defective\n",
        "- Of the defective ones, 50% are from M1 and 50% are from M2\n",
        "\n",
        "What is the probability of a defective part, given that it came from M2:\n",
        "- P(def|M2) = P(M2|def) * P(def) / P(M2)\n",
        "- P(def|M2) = 0.5 * 0.01 / (30/50) = 0.0125\n",
        "\n"
      ],
      "metadata": {
        "id": "-9EyKNuLOK6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
        "    p_not_a = 1 - p_a\n",
        "    p_b = p_b_given_a * p_a + (p_b_given_not_a * p_not_a)\n",
        "    p_a_given_b = p_b_given_a * p_a / p_b\n",
        "\n",
        "    return p_a_given_b"
      ],
      "metadata": {
        "id": "l7pk9YIW9Dma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdQsmCQg07hl",
        "outputId": "1fb6ad2c-506d-4518-dee5-cc9617df854f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.339%\n"
          ]
        }
      ],
      "source": [
        "p_a = 0.0002\n",
        "p_b_given_a = 0.85\n",
        "p_b_given_not_a = 0.05\n",
        "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
        "print(f'{round(result*100,3)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Probabilities"
      ],
      "metadata": {
        "id": "EwBXFLjn6WLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the iris dataset built in to sklearn as an example. We will use the first two columns for our input X. there are 3 possible values for y (0,1,2).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Geo0Wa777XT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "wDJBikPo7bD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were given a new x value, we want to predict which of the three classes it is in.\n",
        "\n",
        "Applying Bayes, it would look something like this:\n",
        "- P(y=0|X) = P(X|y=0) * P(y=0) / P(X)\n",
        "- P(y=1|X) = P(X|y=1) * P(y=1) / P(X)\n",
        "- P(y=2|X) = P(X|y=2) * P(y=2) / P(X)\n",
        "\n",
        "So we would calculate these three probablities and take the maximum one as our prediction. Since P(X) is going to be the same for each calculation, we can simplify our calculations by dropping it.\n",
        "\n",
        "Now let's zoom in a little more on a probability calculation. We can think of X as the set of columns. We have 2 columns, X1 and X1, so our probability equation now looks something like this:\n",
        "- P(y=0|X) = P(y=0|X1,X2) = P(X1|y=0) * P(X2|y=0) * P(y=0) \n",
        "\n",
        "So we have 3 terms to calculate.\n",
        "- P(X1|y=0): What is the probability of some value X1 for the first columns, given that the classification would be y = 0\n",
        "- P(X2|y=0): Same thing, but for X2\n",
        "- P(y=0): Marginal probability of being classified as y = 0. For this one, we simply divide the number of values where y = 0 by the total number of observations. We therefore want to focus on the first two.\n",
        "\n",
        "In order to get a probability for a certain value, we must assume some sort of probability distribution for the values of the column. For simplicity's sake, we will assume normal/Gaussian distribution, although this may not always be a reality.\n",
        "\n",
        "We can do some quick data visualization to see how the data in the columns is distributed. We can see that it is somewhat normally distributed. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMgmBjNF89nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(X[:, 1], bins=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rKcGPEIl8bjn",
        "outputId": "0eb2bff3-f4f7-4000-8554-83ac1a74f5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f79253337d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to calculate a probability for a value x, we can use the probability density function for normal distribution, which is as follows:\n",
        "\n",
        "$$ {\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}} $$\n",
        "\n",
        "Where $\\sigma$ is the standard deviation of the column, and $\\mu$ is the mean.\n",
        "\n",
        "Let's calculate these values for the second column:"
      ],
      "metadata": {
        "id": "3iF2TFPs6czf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean(vals):\n",
        "    return round(sum(vals) / float(len(vals)),3)\n",
        "\n",
        "def std(vals):\n",
        "    m = mean(vals)\n",
        "    v = sum([(x-m)**2 for x in vals]) / float(len(vals) - 1)\n",
        "    return round(math.sqrt(v),3)\n",
        "\n",
        "# Gaussian Probability Density Function\n",
        "def gaussian_pdf(x, mean, stdev):\n",
        "    exponent = math.exp(-((x - mean)**2 / (2 * stdev**2 )))\n",
        "    res = (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
        "    return round(res, 4)"
      ],
      "metadata": {
        "id": "3BTYT_fpoiKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the mean and standard deviation to estimate some probability densities for some values of x. But we're not just interested in the marginal probabilities. We want to know P(X2|y=0). Therefore, we need to take the values from our dataset where y = 0. The data is evenly distributed, so the values end up being about the same. "
      ],
      "metadata": {
        "id": "bcNQY2fPAYFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col2 = X[:,1]\n",
        "col2_y0 = col2[y==0]\n",
        "mu = mean(col2)\n",
        "sd = std(col2)\n",
        "\n",
        "print(f'Mean: {mu}, Std: {sd}')\n",
        "\n",
        "for x in [3.05, 3, 2, 1]:\n",
        "    px = gaussian_pdf(x, mu, sd)\n",
        "    print(f'x: {x}, prob: {px}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gcHJsK6BfkL",
        "outputId": "a21a3407-ac2d-475b-b680-37d22fa52180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 3.057, Std: 0.436\n",
            "x: 3.05, prob: 0.9149\n",
            "x: 3, prob: 0.9072\n",
            "x: 2, prob: 0.0484\n",
            "x: 1, prob: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will want to store the mean and standard deviation values, since we will be using them a lot. To this end, we will define a function that stores the set of all these values. Since we have 2 columns and 3 values for y, we will be storing 6 separate objects. Each object will contain the corresponding mean and standard deviation."
      ],
      "metadata": {
        "id": "24Y4KUJpBvcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_sd_set(X, y):\n",
        "    num_cols = X.shape[1]\n",
        "    y_vals = set(y)\n",
        "    result = np.empty((num_cols,len(y_vals)), dict)\n",
        "\n",
        "    for i in range(num_cols):\n",
        "        x = X[:,i]\n",
        "\n",
        "        for j in y_vals:\n",
        "            xj = x[y==j]\n",
        "            v = {\n",
        "                'm': mean(xj),\n",
        "                's': std(xj)\n",
        "            }\n",
        "\n",
        "            result[i,j] = v\n",
        "        \n",
        "    return result\n"
      ],
      "metadata": {
        "id": "7eJZtYq-xDMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ms_set = get_mean_sd_set(X,y)\n",
        "print(ms_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTNzUU66CgE6",
        "outputId": "cbcbe03a-2ae9-4f04-ec80-ad4ed63c6a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'m': 5.006, 's': 0.352} {'m': 5.936, 's': 0.516}\n",
            "  {'m': 6.588, 's': 0.636}]\n",
            " [{'m': 3.428, 's': 0.379} {'m': 2.77, 's': 0.314}\n",
            "  {'m': 2.974, 's': 0.322}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "txRiJBIyCupv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a concrete example. Suppose we get a new value of [5.7, 2.8]. To decide which class it belongs in, we need to use Bayes for each of the 3 possible y values and take the max:\n",
        "\n",
        "- P(y=0|X1,X2) = P(X1|y=0) * P(X1|y=0) * P(y=0)\n",
        "- P(y=1|X1,X2) = ...\n",
        "- P(y=2|X1,X2) = ...\n",
        "\n",
        "We have a probability for each y value. We take the max to get our final prediction."
      ],
      "metadata": {
        "id": "djiFcBxAzgNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_x = np.array([5.7, 2.8])\n",
        "\n",
        "y_vals = list(set(y))\n",
        "\n",
        "probs = []\n",
        "\n",
        "for j in range(len(y_vals)):\n",
        "    p_y = len(X[y==y_vals[j]]) / len(X)\n",
        "    total = p_y\n",
        "\n",
        "    for i in range(len(new_x)):\n",
        "        d = ms_set[i,j]\n",
        "        next_prob = gaussian_pdf(new_x[i], d['m'], d['s'])\n",
        "        total *= next_prob\n",
        "    \n",
        "    probs.append(total)\n",
        "\n",
        "print(probs)\n",
        "\n",
        "i = np.argmax(probs)\n",
        "pred_y = y_vals[i]\n",
        "\n",
        "print(f'Prediction: {pred_y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSShA3s702fG",
        "outputId": "382f17dd-a5cc-4a19-f3dd-1d9cbc8f9be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.014428469999999999, 0.29357902666666663, 0.08447033999999999]\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Class"
      ],
      "metadata": {
        "id": "3yXoeEzEEHCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can put all this theory together, and use some of the previously defined functions to build our custom Naive Bayes classifier."
      ],
      "metadata": {
        "id": "XOF7JqYNLk4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.__ms_set = None\n",
        "        self.__y_probs = {}\n",
        "        self.__y_vals = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.__ms_set = get_mean_sd_set(X,y)\n",
        "        # Capture the y probabilities\n",
        "        self.__y_vals = list(set(y))\n",
        "        for yv in self.__y_vals:\n",
        "            p_y = len(X[y==yv]) / len(X)\n",
        "            self.__y_probs[yv] = p_y\n",
        "    \n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(x) for x in X]\n",
        "\n",
        "\n",
        "    def _predict(self, x):\n",
        "        probs = []\n",
        "        for j in range(len(self.__y_vals)):\n",
        "            yv = self.__y_vals[j]\n",
        "            p_y = self.__y_probs[yv]\n",
        "            total = p_y\n",
        "\n",
        "            for i in range(len(x)):\n",
        "                d = self.__ms_set[i,j]\n",
        "                next_prob = gaussian_pdf(x[i], d['m'], d['s'])\n",
        "                total *= next_prob\n",
        "            \n",
        "            probs.append(total)\n",
        "\n",
        "        max_ind = np.argmax(probs)\n",
        "        pred_y = self.__y_vals[max_ind]\n",
        "\n",
        "        return pred_y\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aWkXUzgR1Tt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Comparison"
      ],
      "metadata": {
        "id": "vKTHM-1rGSj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "nvPwP32FGYrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "Y-ZtB2NiGTqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_nb = MyNaiveBayes()\n",
        "\n",
        "my_nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = my_nb.predict(X_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XnI95JGmqw",
        "outputId": "26971568-6623-411f-9c53-774e21f7832e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.91      1.00      0.95        10\n",
            "           2       1.00      0.90      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison with sklearn\n",
        "nb = GaussianNB()\n",
        "\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred2 = nb.predict(X_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDeRo54YG4fw",
        "outputId": "357c1aa9-bbc9-4fce-904b-a87692af9779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        10\n",
            "           2       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}